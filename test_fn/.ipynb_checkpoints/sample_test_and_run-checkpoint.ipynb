{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io_ import readFiles\n",
    "import numpy as np\n",
    "\n",
    "# This is a sample test folder\n",
    "test_folder = 'test_images'\n",
    "image_paths = readFiles(test_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3088"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score each of the images based on the likelihood it belongs to a class. Larger scores indicate higher likelihood.\n",
    "\n",
    "Classes are arranged alphabeticall, [1_Capitol, 1_cemetry,..., 1_Underground, americano, ... taco]. Screen-shot is attached.\n",
    "\n",
    "The first thirteen belong to buidings and the next fifty nine belong to food, this makes for a total of 72 classes.\n",
    "\n",
    "Scores should take the form of a [Nx72] array, where N is the number of test images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES = [\n",
    "    '1_Capitol',\n",
    "    '1_cemetry',\n",
    "    '1_Chijmes',\n",
    "    '1_church',\n",
    "    '1_Cityhall',\n",
    "    '1_CoffeeShop',\n",
    "    '1_HDB',\n",
    "    '1_law',\n",
    "    '1_library',\n",
    "    '1_museum',\n",
    "    '1_scis',\n",
    "    '1_SMUadmin',\n",
    "    '1_Underground',\n",
    "    'americano',\n",
    "    'bak_kwa',\n",
    "    'beef_noodles_soup',\n",
    "    'boiled_clams',\n",
    "    'boonthi',\n",
    "    'braised_pig_skin',\n",
    "    'braised_pig_trotter_with_mushroom',\n",
    "    'braised_pork_with_black_fungus',\n",
    "    'cafe_coffee_with_milk',\n",
    "    'chasoba',\n",
    "    'chicken_and_mushroom_noodles_soup',\n",
    "    'chicken_feet_noodles_soup',\n",
    "    'chicken_macaroni_soup',\n",
    "    'cooked_jellyfish_with_sesame_seed',\n",
    "    'crab_stick',\n",
    "    'curry_udon',\n",
    "    'dry_chicken_feet_noodles',\n",
    "    'dry_duck_noodles',\n",
    "    'duck_noodles_soup',\n",
    "    'espresso',\n",
    "    'fish_tofu_cheese_tofu',\n",
    "    'fishball',\n",
    "    'fried_fishcake',\n",
    "    'fried_spicy_mussels',\n",
    "    'fried_vegetable_samosa',\n",
    "    'green_bean_cake',\n",
    "    'idli',\n",
    "    'instant_coffee',\n",
    "    'jalebi',\n",
    "    'kebab_wrap',\n",
    "    'kopi_o',\n",
    "    'local_coffee_with_milk',\n",
    "    'lor_bak_nonya_style_simmered_pork',\n",
    "    'lotus_paste_with_salted_egg_pastry',\n",
    "    'muruku',\n",
    "    'onion_bhaji',\n",
    "    'otah',\n",
    "    'pig_trotter_in_vinegar',\n",
    "    'pineapple_tart',\n",
    "    'pong_piah',\n",
    "    'prawn_tempura_udon_soup',\n",
    "    'putu_mayam',\n",
    "    'raw_oyster',\n",
    "    'red_bean_cake',\n",
    "    'red_wine_chicken_mee_sua',\n",
    "    'roasted_pork_belly',\n",
    "    'roasted_suckling_pig',\n",
    "    'sa_kei_ma',\n",
    "    'sambal_clams',\n",
    "    'sesame_seed_ball',\n",
    "    'shredded_chicken_and_mushroom_noodles',\n",
    "    'soup_with_mutton',\n",
    "    'soya_sauce_chicken_noodles',\n",
    "    'steamed_minced_pork_with_fish_mushroom_and_preserved_veg',\n",
    "    'stir_fried_pork',\n",
    "    'stir_fried_udon',\n",
    "    'sup_tulang',\n",
    "    'sweet_and_sour_meat',\n",
    "    'taco',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test function\n",
    "The cells below will read in the model file and weights (trained using EfficientNetB5).  The model file utilises a pipeline which will perform the necessary preprocessing, so no additional cleaning needs to be done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Write your test function here ##############\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Below is a possible mechanics for score computation;\n",
    "# However, it must be modifed according to pre-processing amd network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and weights from json and h5 files\n",
    "MODEL_FILE = 'GerryChng_EfficientNetB5_20210410_01.json'\n",
    "MODEL_WEIGHTS = 'GerryChng_EfficientNetB5_20210410_01.h5'\n",
    "\n",
    "# Model reconstruction from JSON file\n",
    "with open(MODEL_FILE, 'r') as json_file:\n",
    "    json_savedModel= json_file.read()\n",
    "\n",
    "test_model = tf.keras.models.model_from_json(json_savedModel)\n",
    "test_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Load weights into the new model\n",
    "test_model.load_weights(MODEL_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define my own ground truth array based on the actual directory the file was from\n",
    "image_classes = [x.split('___')[0] for x in image_paths]\n",
    "my_test_gt = [CLASS_NAMES.index(x) for x in image_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = len(image_paths)\n",
    "\n",
    "# Training image dimensions - 456x456 because we are loading EfficientNetB5\n",
    "IMG_SIZE = 456\n",
    "IMG_CHANNELS = 3\n",
    "\n",
    "scores = np.zeros([num_images, 72])\n",
    "image_file = [None] * num_images\n",
    "\n",
    "for i,im in enumerate(image_paths):\n",
    "    img = image.load_img(test_folder + '/' + im, target_size=(456, 456))\n",
    "    img = image.img_to_array(img)\n",
    "    img = img.reshape([1,456,456,3])\n",
    "    scores[i] = test_model.predict(img)\n",
    "    image_file[i] = im\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 2 2 9 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "results = np.argmax(scores[:20], axis=1)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrating the evaluation functions\n",
    "\n",
    "You need not modify code beyond this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall accuracy\n",
      "percentage top1 accuracy: 0.9815414507772021\n",
      "percentage top5 accuracy: 0.9993523316062176\n"
     ]
    }
   ],
   "source": [
    "from io_ import score\n",
    "print('overall accuracy')\n",
    "\n",
    "# simulated scores\n",
    "#scores = np.zeros([2,72])\n",
    "#scores[0,0] = 1000\n",
    "#scores[1,3] = 1000\n",
    "#scores[1,13] = 999\n",
    "\n",
    "# Ground-truth: the first image belongs to class 0; the second image to class 13.\n",
    "\"\"\"\n",
    "Please change this to reflect your actual ground truth \"\"\"\n",
    "#gt = np.array([0,13]) \n",
    "gt = np.array(my_test_gt)\n",
    "\n",
    "top1, top5 = score(scores, gt, 5)\n",
    "print('percentage top1 accuracy:', top1)\n",
    "print('percentage top5 accuracy:', top5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on building only\n",
      "percentage top1 accuracy: 0.9801324503311258\n",
      "percentage top5 accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "print('accuracy on building only')\n",
    "mask = gt<13\n",
    "top1, top5 = score(scores[mask,:13], gt[mask], 5)\n",
    "print('percentage top1 accuracy:', top1)\n",
    "print('percentage top5 accuracy:', top5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on food only\n",
      "percentage top1 accuracy: 0.9816138917262512\n",
      "percentage top5 accuracy: 0.9993190330268982\n"
     ]
    }
   ],
   "source": [
    "print('accuracy on food only')\n",
    "mask = gt>=13\n",
    "top1, top5 = score(scores[mask,13:], gt[mask]-13, 5)\n",
    "print('percentage top1 accuracy:', top1)\n",
    "print('percentage top5 accuracy:', top5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_Chijmes___2100.jpg was wrongly classified as 1_museum\n",
      "1_Underground___1782.JPG was wrongly classified as 1_library\n",
      "1_scis___0037.jpg was wrongly classified as 1_SMUadmin\n",
      "americano___0792.jpg was wrongly classified as kopi_o\n",
      "americano___0808.jpg was wrongly classified as kopi_o\n",
      "bak_kwa___1964.jpg was wrongly classified as roasted_suckling_pig\n",
      "beef_noodles_soup___2170.jpg was wrongly classified as putu_mayam\n",
      "beef_noodles_soup___2173.jpg was wrongly classified as steamed_minced_pork_with_fish_mushroom_and_preserved_veg\n",
      "boiled_clams___0488.jpg was wrongly classified as shredded_chicken_and_mushroom_noodles\n",
      "braised_pig_skin___2416.jpg was wrongly classified as stir_fried_pork\n",
      "braised_pork_with_black_fungus___0206.jpg was wrongly classified as stir_fried_pork\n",
      "braised_pork_with_black_fungus___0216.jpg was wrongly classified as roasted_suckling_pig\n",
      "braised_pork_with_black_fungus___0221.jpg was wrongly classified as lor_bak_nonya_style_simmered_pork\n",
      "cafe_coffee_with_milk___0644.jpg was wrongly classified as local_coffee_with_milk\n",
      "cafe_coffee_with_milk___0654.jpg was wrongly classified as espresso\n",
      "cafe_coffee_with_milk___0678.jpg was wrongly classified as espresso\n",
      "cafe_coffee_with_milk___0680.jpg was wrongly classified as local_coffee_with_milk\n",
      "cafe_coffee_with_milk___0685.jpg was wrongly classified as espresso\n",
      "curry_udon___0756.jpeg was wrongly classified as soup_with_mutton\n",
      "curry_udon___0771.jpg was wrongly classified as beef_noodles_soup\n",
      "dry_duck_noodles___2034.jpg was wrongly classified as beef_noodles_soup\n",
      "espresso___0140.jpg was wrongly classified as kopi_o\n",
      "espresso___0151.jpg was wrongly classified as kopi_o\n",
      "fish_tofu_cheese_tofu___1843.jpg was wrongly classified as otah\n",
      "fish_tofu_cheese_tofu___1857.jpg was wrongly classified as fried_fishcake\n",
      "fish_tofu_cheese_tofu___1876.jpg was wrongly classified as lor_bak_nonya_style_simmered_pork\n",
      "fried_fishcake___2991.jpg was wrongly classified as fishball\n",
      "instant_coffee___3068.jpg was wrongly classified as americano\n",
      "jalebi___2591.jpg was wrongly classified as muruku\n",
      "jalebi___2603.jpg was wrongly classified as muruku\n",
      "kopi_o___2622.jpg was wrongly classified as americano\n",
      "kopi_o___2633.jpg was wrongly classified as instant_coffee\n",
      "kopi_o___2661.jpg was wrongly classified as americano\n",
      "local_coffee_with_milk___2542.jpg was wrongly classified as instant_coffee\n",
      "lor_bak_nonya_style_simmered_pork___1417.jpg was wrongly classified as braised_pork_with_black_fungus\n",
      "lor_bak_nonya_style_simmered_pork___1483.jpg was wrongly classified as braised_pig_skin\n",
      "lotus_paste_with_salted_egg_pastry___2446.jpg was wrongly classified as pong_piah\n",
      "muruku___0578.jpg was wrongly classified as green_bean_cake\n",
      "otah___1046.jpg was wrongly classified as red_wine_chicken_mee_sua\n",
      "pong_piah___0236.jpg was wrongly classified as lotus_paste_with_salted_egg_pastry\n",
      "pong_piah___0287.jpg was wrongly classified as fishball\n",
      "pong_piah___0350.jpg was wrongly classified as sa_kei_ma\n",
      "prawn_tempura_udon_soup___2086.jpg was wrongly classified as chicken_feet_noodles_soup\n",
      "sambal_clams___0410.jpg was wrongly classified as boiled_clams\n",
      "shredded_chicken_and_mushroom_noodles___1297.jpg was wrongly classified as chicken_and_mushroom_noodles_soup\n",
      "soup_with_mutton___1680.jpg was wrongly classified as sup_tulang\n",
      "soup_with_mutton___1696.jpg was wrongly classified as sup_tulang\n",
      "soup_with_mutton___1717.jpg was wrongly classified as steamed_minced_pork_with_fish_mushroom_and_preserved_veg\n",
      "soup_with_mutton___1727.jpg was wrongly classified as steamed_minced_pork_with_fish_mushroom_and_preserved_veg\n",
      "soup_with_mutton___1728.jpg was wrongly classified as lor_bak_nonya_style_simmered_pork\n",
      "soup_with_mutton___1735.jpg was wrongly classified as sup_tulang\n",
      "soya_sauce_chicken_noodles___2706.jpg was wrongly classified as dry_duck_noodles\n",
      "stir_fried_pork___0887.jpg was wrongly classified as fried_spicy_mussels\n",
      "stir_fried_pork___0921.jpg was wrongly classified as steamed_minced_pork_with_fish_mushroom_and_preserved_veg\n",
      "sweet_and_sour_meat___2892.jpg was wrongly classified as soup_with_mutton\n",
      "sweet_and_sour_meat___2934.jpg was wrongly classified as lor_bak_nonya_style_simmered_pork\n",
      "taco___1214.jpg was wrongly classified as kebab_wrap\n"
     ]
    }
   ],
   "source": [
    "# Lists the files that were wrongly classified for manual checking\n",
    "for i in range(len(scores)):\n",
    "    pred_idx = np.argmax(scores[i])\n",
    "    actual_idx = gt[i]\n",
    "    if pred_idx != actual_idx:\n",
    "        print('{} was wrongly classified as {}'.format(image_file[i], CLASS_NAMES[pred_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
